<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Huc&#39;s Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-10-10T03:21:18.710Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>tmux</title>
    <link href="http://example.com/2024/10/10/tmux/"/>
    <id>http://example.com/2024/10/10/tmux/</id>
    <published>2024-10-09T16:00:00.000Z</published>
    <updated>2024-10-10T03:21:18.710Z</updated>
    
    <content type="html"><![CDATA[<h1 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h1><h2 id="在后台保存会话状态"><a href="#在后台保存会话状态" class="headerlink" title="在后台保存会话状态"></a>在后台保存会话状态</h2><p>使用场景：做实验时会有比较麻烦的环境配置和启动，或者每次都需要重新启动一些服务，每次登陆服务器这些步骤都需要重新做一遍很麻烦。</p><p>可以使用 tmux 在断开服务器前将会话保存在后台，下次登陆时再重连这个 tmux 会话。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新建一个 tmux session</span></span><br><span class="line">tmux </span><br><span class="line">tmux new -s mysession # 自定义session名称</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分离 tmux session，后台运行</span></span><br><span class="line">Ctrl + b, 然后按 d</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看已存在的 tmux 会话</span></span><br><span class="line">tmux ls</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新连接到一个 tmux 会话</span></span><br><span class="line">tmux attach-session -t 0</span><br><span class="line">tmux attach-session -t mysession</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">关闭 tmux session</span> </span><br><span class="line">tmux kill-session -t mysession</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;tmux&quot;&gt;&lt;a href=&quot;#tmux&quot; class=&quot;headerlink&quot; title=&quot;tmux&quot;&gt;&lt;/a&gt;tmux&lt;/h1&gt;&lt;h2 id=&quot;在后台保存会话状态&quot;&gt;&lt;a href=&quot;#在后台保存会话状态&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    
    <category term="devtools" scheme="http://example.com/tag/devtools/"/>
    
  </entry>
  
  <entry>
    <title>Python 常见问题与总结</title>
    <link href="http://example.com/2024/09/20/python/"/>
    <id>http://example.com/2024/09/20/python/</id>
    <published>2024-09-19T16:00:00.000Z</published>
    <updated>2024-09-20T12:01:19.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-常见问题与总结"><a href="#Python-常见问题与总结" class="headerlink" title="Python 常见问题与总结"></a>Python 常见问题与总结</h1><h2 id="python-运行脚本与运行模块"><a href="#python-运行脚本与运行模块" class="headerlink" title="python 运行脚本与运行模块"></a>python 运行脚本与运行模块</h2><p>在如下的场景，工作目录为 root，<code>python src/script.py</code> 和 <code>python -m src.script</code> 有什么区别？这两种情况下如何 <code>import</code> 其他模块。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root/</span><br><span class="line">├── src/</span><br><span class="line">│   ├── script.py</span><br><span class="line">│   ├── utils.py</span><br><span class="line">├── helpers/</span><br><span class="line">│   └── submodule.py</span><br></pre></td></tr></table></figure><p>一些直接的结论有：</p><ul><li>运行脚本会把脚本所在目录（上述场景中的src）添加到 <code>sys.path[0]</code>，运行模块都会将工作目录（对应上述场景的root）添加到 <code>sys.path[0]</code> </li><li>import 的模块必须在 <code>sys.path</code> 中，而 <code>sys.path</code> 是由 <code>sys.path[0]</code> 和 <code>PYTHONPATH</code> 等决定的</li><li>运行模块时会有包的上下文，而相对导入必须在包的上下文中进行（如 from .utils import some_function），且相对导入必须使用 from … import … 的形式，不能使用 import … 的形式</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Python-常见问题与总结&quot;&gt;&lt;a href=&quot;#Python-常见问题与总结&quot; class=&quot;headerlink&quot; title=&quot;Python 常见问题与总结&quot;&gt;&lt;/a&gt;Python 常见问题与总结&lt;/h1&gt;&lt;h2 id=&quot;python-运行脚本与运行模块&quot;</summary>
      
    
    
    
    
    <category term="python" scheme="http://example.com/tag/python/"/>
    
  </entry>
  
  <entry>
    <title>Hexo + Vercel 搭建个人博客</title>
    <link href="http://example.com/2024/08/31/Hexo-Vercel-Blog/"/>
    <id>http://example.com/2024/08/31/Hexo-Vercel-Blog/</id>
    <published>2024-08-30T16:00:00.000Z</published>
    <updated>2024-08-31T05:58:23.443Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hexo-Vercel-搭建个人博客"><a href="#Hexo-Vercel-搭建个人博客" class="headerlink" title="Hexo + Vercel 搭建个人博客"></a>Hexo + Vercel 搭建个人博客</h1><h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><p>本文记录了自己使用 Hexo + Vercel 搭建个人博客的过程。</p><p>首先是安装 hexo 并初始化项目：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo --version  # 验证是否安装成功</span><br><span class="line"></span><br><span class="line">hexo init myBlog</span><br><span class="line">cd myBlog</span><br><span class="line">npm i</span><br></pre></td></tr></table></figure><p>安装主题可以去 <a href="https://hexo.io/themes/">hexo-theme</a> 查看，我选择了 <a href="https://github.com/xbmlz/hexo-theme-maple">maple</a>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd your-blog/themes</span><br><span class="line">git clone https://github.com/xbmlz/hexo-theme-maple.git themes/maple</span><br></pre></td></tr></table></figure><p>修改相关的配置 _config.yml:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">theme:</span> <span class="string">some-theme</span></span><br><span class="line"><span class="string">+</span> <span class="attr">theme:</span> <span class="string">maple</span></span><br></pre></td></tr></table></figure><p>添加其他页面如 <code>tag</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page tags</span><br></pre></td></tr></table></figure><p>source&#x2F;tag&#x2F;index.md 中的 front-matter 需要添加 layout：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">tag</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">tag</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>注意 themes&#x2F;maple&#x2F;_config.yml 中也需要修改相关配置，一些自定义的样式也可以修改 themes&#x2F;maple 里面的内容。</p><h2 id="部署到-Vercel"><a href="#部署到-Vercel" class="headerlink" title="部署到 Vercel"></a>部署到 Vercel</h2><p>在 _config.yml 中添加部署配置：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">ssh...</span>  <span class="comment"># 你的仓库地址</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">main</span></span><br></pre></td></tr></table></figure><p>然后安装 hexo-deployer-git 插件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>将生成的静态文件上传到 github：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><p>然后在 vercel 中导入项目，选择 github 仓库，Framework Preset 选择 <code>Others</code>，部署命令空着即可，因为 hexo 生成的静态文件已经上传到 github 了，然后点击 Deploy 即可。</p><h2 id="添加额外功能"><a href="#添加额外功能" class="headerlink" title="添加额外功能"></a>添加额外功能</h2><h3 id="RSS"><a href="#RSS" class="headerlink" title="RSS"></a>RSS</h3><p>安装 hexo-generator-feed 插件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-feed --save</span><br></pre></td></tr></table></figure><p>_config.yml 中添加：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">feed:</span></span><br><span class="line">  <span class="attr">type:</span> </span><br><span class="line">    <span class="bullet">-</span> <span class="string">atom</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">rss2</span></span><br><span class="line">  <span class="attr">path:</span> </span><br><span class="line">    <span class="bullet">-</span> <span class="string">atom.xml</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">rss2.xml</span></span><br><span class="line">  <span class="attr">limit:</span> <span class="number">20</span></span><br><span class="line">  <span class="attr">order_by:</span> <span class="string">-date</span></span><br><span class="line">  <span class="attr">autodiscovery:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="域名"><a href="#域名" class="headerlink" title="域名"></a>域名</h2><p>TODO</p><h2 id="vscode-添加-md-代码片段"><a href="#vscode-添加-md-代码片段" class="headerlink" title="vscode 添加 md 代码片段"></a>vscode 添加 md 代码片段</h2><p>每次写 blog 的时候都要在开头添加一些配置信息，这个时候就可以使用 vscode 的代码片段功能，快速添加配置信息。</p><p>首先 <code>cmd + shift + p</code> 输入 <code>settings</code>，选择 <code>Preferences: Open Settings (json)</code>，然后添加如下代码：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;[markdown]&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;editor.quickSuggestions&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure><p>然后 <code>cmd + shift + p</code> 输入 <code>snippets</code>，选择 <code>Preferences: Configure User Snippets</code>，然后选择 <code>markdown.json</code>，添加如下代码：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;create a new blog post&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;prefix&quot;</span><span class="punctuation">:</span> <span class="string">&quot;blog&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;body&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line"><span class="string">&quot;---&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&quot;title: &quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&quot;date: $&#123;CURRENT_YEAR&#125;/$&#123;CURRENT_MONTH&#125;/$&#123;CURRENT_DATE&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&quot;tags:&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&quot;  - &quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&quot;---&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Create a new blog post&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>这样以后在 markdown 文件中输入 <code>blog</code> 就可以快速添加配置信息了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hexo-Vercel-搭建个人博客&quot;&gt;&lt;a href=&quot;#Hexo-Vercel-搭建个人博客&quot; class=&quot;headerlink&quot; title=&quot;Hexo + Vercel 搭建个人博客&quot;&gt;&lt;/a&gt;Hexo + Vercel 搭建个人博客&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    
    <category term="others" scheme="http://example.com/tag/others/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2024/08/29/hello-world/"/>
    <id>http://example.com/2024/08/29/hello-world/</id>
    <published>2024-08-29T00:05:21.457Z</published>
    <updated>2024-08-31T05:41:46.209Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
    <category term="test" scheme="http://example.com/tag/test/"/>
    
  </entry>
  
  <entry>
    <title>使用 Hugging Face Transformers Fine-Tune BERT</title>
    <link href="http://example.com/2024/06/16/FineTune-Bert/"/>
    <id>http://example.com/2024/06/16/FineTune-Bert/</id>
    <published>2024-06-15T16:00:00.000Z</published>
    <updated>2024-08-31T05:10:53.089Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Fine-Tuning-BERT-using-Hugging-Face-Transformers"><a href="#Fine-Tuning-BERT-using-Hugging-Face-Transformers" class="headerlink" title="Fine-Tuning BERT using Hugging Face Transformers"></a>Fine-Tuning BERT using Hugging Face Transformers</h1><p><a href="https://learnopencv.com/fine-tuning-bert/">原文链接</a></p><p>本篇文章使用 Hugging Face Transformers Fine-Tune BERT，获得一个能够对 Arxiv 文章摘要进行分类的模型，一共 11 个类别。</p><p>本篇文章包含以下内容：</p><ul><li>安装依赖</li><li>解释用来 Fine-Tune BERT 的数据集</li><li>模型和训练</li><li>模型测试和评估</li></ul><blockquote><p>本文大部分用到的 api 在 <code>transformer</code> 的<a href="https://huggingface.co/docs/transformers/index">官方文档</a>都有，这里仅转载一下主要的处理流程。</p><p>如果实践中数据集是没有 label 的，则需要使用无监督的方法，比如 Masked Language Modeling（MLM） 以及 Next Sentence Prediction（NSP）。</p><p>为了构造 MLM 和 NSP 的数据，可以使用 transformers 库中的 <code>DataCollatorForLanguageModeling</code> 和 <code>DataCollatorForNextSentencePrediction</code>。</p></blockquote><h2 id="Why-Fine-Tuning-BERT-Matters"><a href="#Why-Fine-Tuning-BERT-Matters" class="headerlink" title="Why Fine-Tuning BERT Matters?"></a>Why Fine-Tuning BERT Matters?</h2><p>Fine-Tuning 的目的是为了让 BERT 从一个通用的工具变成一个特定任务的工具。</p><h2 id="Hugging-Face-Transformers"><a href="#Hugging-Face-Transformers" class="headerlink" title="Hugging Face Transformers"></a>Hugging Face Transformers</h2><p>本文章使用 Hugging Face Transformers，一个用于自然语言处理的库。</p><h2 id="Fine-Tuning-BERT-on-the-Arxiv-Abstract-Classification-Dataset"><a href="#Fine-Tuning-BERT-on-the-Arxiv-Abstract-Classification-Dataset" class="headerlink" title="Fine-Tuning BERT on the Arxiv Abstract Classification Dataset"></a>Fine-Tuning BERT on the Arxiv Abstract Classification Dataset</h2><p>首先，我们需要安装一些依赖。</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!pip install -U transformers datasets evaluate accelerate</span><br><span class="line">!pip install scikit-learn</span><br><span class="line">!pip install tensorboard</span><br></pre></td></tr></table></figure><h3 id="Importing-the-Necessary-Libraries"><a href="#Importing-the-Necessary-Libraries" class="headerlink" title="Importing the Necessary Libraries"></a>Importing the Necessary Libraries</h3><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    AutoTokenizer,</span><br><span class="line">    DataCollatorWithPadding,</span><br><span class="line">    AutoModelForSequenceClassification,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">    Trainer,</span><br><span class="line">    pipeline,</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h3 id="Setting-the-Hyperparameters-for-Fine-Tuning-BERT"><a href="#Setting-the-Hyperparameters-for-Fine-Tuning-BERT" class="headerlink" title="Setting the Hyperparameters for Fine-Tuning BERT"></a>Setting the Hyperparameters for Fine-Tuning BERT</h3><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">NUM_PROCS = <span class="number">32</span></span><br><span class="line">LR = <span class="number">0.00005</span></span><br><span class="line">EPOCHS = <span class="number">5</span></span><br><span class="line">MODEL = <span class="string">&#x27;distilbert-base-uncased&#x27;</span></span><br><span class="line">OUT_DIR = <span class="string">&#x27;arxiv_bert&#x27;</span></span><br></pre></td></tr></table></figure><ul><li>MODEL：表示我们要使用的模型，这里使用的是 <code>bert-base-uncased</code>。这里的 <code>uncased</code> 表示该模型是在小写的文本上训练的。</li><li>OUT_DIR：表示我们训练好的模型的保存路径。</li></ul><h3 id="Download-and-Prepare-the-Dataset"><a href="#Download-and-Prepare-the-Dataset" class="headerlink" title="Download and Prepare the Dataset"></a>Download and Prepare the Dataset</h3><p>使用 <code>datasets</code> 库下载数据集 arxiv 的分类数据集。</p><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = load_dataset(<span class="string">&quot;ccdv/arxiv-classification&quot;</span>, split=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">valid_dataset = load_dataset(<span class="string">&quot;ccdv/arxiv-classification&quot;</span>, split=<span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line">test_dataset = load_dataset(<span class="string">&quot;ccdv/arxiv-classification&quot;</span>, split=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_dataset)</span><br><span class="line"><span class="built_in">print</span>(valid_dataset)</span><br><span class="line"><span class="built_in">print</span>(test_dataset)</span><br></pre></td></tr></table></figure><blockquote><p>下载数据可能需要为 load_dataset 加一个 trust_remote_code&#x3D;True 的参数。</p></blockquote><figure class="highlight cmd"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line"><span class="function">    features: [&#x27;<span class="title">text</span>&#x27;, &#x27;<span class="title">label</span>&#x27;],</span></span><br><span class="line"><span class="function">    <span class="title">num_rows</span>: 28388</span></span><br><span class="line"><span class="function">&#125;)</span></span><br><span class="line"><span class="function"><span class="title">Dataset</span>(&#123;</span></span><br><span class="line"><span class="function">    <span class="title">features</span>: [&#x27;<span class="title">text</span>&#x27;, &#x27;<span class="title">label</span>&#x27;],</span></span><br><span class="line"><span class="function">    <span class="title">num_rows</span>: 2500</span></span><br><span class="line"><span class="function">&#125;)</span></span><br><span class="line"><span class="function"><span class="title">Dataset</span>(&#123;</span></span><br><span class="line"><span class="function">    <span class="title">features</span>: [&#x27;<span class="title">text</span>&#x27;, &#x27;<span class="title">label</span>&#x27;],</span></span><br><span class="line"><span class="function">    <span class="title">num_rows</span>: 2500</span></span><br><span class="line"><span class="function">&#125;)</span></span><br></pre></td></tr></table></figure><p>查看一下 dataset 的格式，可以看到每个样本有两个字段，一个是 text，一个是 label。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Constrained Submodular Maximization ... and A.14.\n\n22\n\n\x0c&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;label&#x27;</span>: <span class="number">8</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="Dataset-Label-Information"><a href="#Dataset-Label-Information" class="headerlink" title="Dataset Label Information"></a>Dataset Label Information</h3><p>定义两个字典，用来将 label 转换为对应的类别，以及将类别转换为对应的 label。</p><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">id2label = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;math.AC&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;cs.CV&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;cs.AI&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;cs.SY&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;math.GR&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;cs.CE&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;cs.PL&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;cs.IT&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;cs.DS&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;cs.NE&quot;</span>,</span><br><span class="line">    <span class="number">10</span>: <span class="string">&quot;math.ST&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">label2id = &#123;</span><br><span class="line">    <span class="string">&quot;math.AC&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;cs.CV&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;cs.AI&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;cs.SY&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;math.GR&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;cs.CE&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;cs.PL&quot;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&quot;cs.IT&quot;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&quot;cs.DS&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&quot;cs.NE&quot;</span>: <span class="number">9</span>,</span><br><span class="line">    <span class="string">&quot;math.ST&quot;</span>: <span class="number">10</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Tokenizing-the-Dataset"><a href="#Tokenizing-the-Dataset" class="headerlink" title="Tokenizing the Dataset"></a>Tokenizing the Dataset</h3><p>使用 <code>AutoTokenizer</code> 实例化一个 tokenizer，并对不同的 dataset 进行 tokenization。</p><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">tokenizer, examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;text&quot;</span>],</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">partial_tokenize_function = partial(preprocess_function, tokenizer)</span><br><span class="line"></span><br><span class="line">tokenized_train = train_dataset.<span class="built_in">map</span>(</span><br><span class="line">    partial_tokenize_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    num_proc=NUM_PROCS</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">tokenized_valid = valid_dataset.<span class="built_in">map</span>(</span><br><span class="line">    partial_tokenize_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    num_proc=NUM_PROCS</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">tokenized_test = test_dataset.<span class="built_in">map</span>(</span><br><span class="line">    partial_tokenize_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    num_proc=NUM_PROCS</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure><p>给 <code>AutoTokenizer</code> 传递的模型名能够初始化专门针对该模型的 tokenizer。</p><p><code>DataCollatorWithPadding</code> 会在训练时动态地将不同长度的 token 转换为相同长度的 tensor。</p><blockquote><p>Transformers 的 <code>Tokenizer</code> 支持 truncation 和 padding 参数，具体查看 <a href="https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">transformers.PreTrainedTokenizer</a></p><p>tokenizer 和 DataCollatorWithPadding 的 padding 之间的区别：</p><ul><li>tokenizer 的 padding 是在 tokenization 时使用，用来将不同长度的文本转换为相同长度的 token。</li><li>DataCollatorWithPadding 的 padding 是在 batch 生成时动态地来将不同长度的 token 转换为相同长度的 tensor。</li></ul><p>DataCollatorWithPadding 传入 tokenizer 需要利用 tokenzier 的信息进行正确地 padding </p><p>使用原文中的代码我遇到了一个 Tokenizer is not defined 的报错，按照 <a href="https://discuss.huggingface.co/t/tokenizer-is-not-defined/39231">这里</a> 的解决方法，用 partial 函数解决了这个问题。</p></blockquote><h3 id="Sample-Tokenization-Example"><a href="#Sample-Tokenization-Example" class="headerlink" title="Sample Tokenization Example"></a>Sample Tokenization Example</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenized_sample = partial_tokenize_function(train_dataset[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(tokenized_sample)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of tokenized IDs: <span class="subst">&#123;<span class="built_in">len</span>(tokenized_sample.input_ids)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of attention mask: <span class="subst">&#123;<span class="built_in">len</span>(tokenized_sample.attention_mask)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">101</span>, <span class="number">14924</span>, <span class="number">2635</span>, <span class="number">24421</span>, <span class="number">13997.</span>.. <span class="number">102</span>], </span><br><span class="line"><span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ... <span class="number">0</span>],</span><br><span class="line"><span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ... <span class="number">1</span>]&#125;</span><br><span class="line">Length of tokenized IDs: <span class="number">512</span></span><br><span class="line">Length of attention mask: <span class="number">512</span></span><br></pre></td></tr></table></figure><ul><li>input_ids：表示 tokenized 后的文本，每个 token 都有一个对应的 id。例如，101 表示 [CLS]，表示序列的开始；102 表示 [SEP]，表示序列的结束。</li><li>token_type_ids：表示 token 的类型，BERT 用于区分两个句子的 token。</li><li>attention_mask：表示哪些 token 是 padding 的，哪些是真实的 token。</li></ul><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><p>定义一个函数，用来计算模型的评估指标。</p><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">accuracy = evaluate.load(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_pred</span>):</span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> accuracy.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure><h3 id="Preparing-the-BERT-Model"><a href="#Preparing-the-BERT-Model" class="headerlink" title="Preparing the BERT Model"></a>Preparing the BERT Model</h3><p>实例化一个 11 分类的 BERT 模型，最后的 BERT 模型有 0.67 亿参数。</p><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModelForSequenceClassification.from_pretrained(</span><br><span class="line">    MODEL,</span><br><span class="line">    num_labels=<span class="number">11</span>,</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id,</span><br><span class="line">)</span><br><span class="line">model.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters())</span><br><span class="line">trainable_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nTotal parameters: <span class="subst">&#123;total_params&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Trainable parameters: <span class="subst">&#123;trainable_params&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">dummy_input = tokenizer(<span class="string">&quot;This is a dummy input&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">dummy_input_dict = &#123;k: v.to(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">for</span> k, v <span class="keyword">in</span> dummy_input.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">summary(model, input_data=dummy_input_dict)</span><br></pre></td></tr></table></figure><ul><li>num_labels：表示分类任务中的类别数。transformers 在加载预训练模型的权重时，会根据 num_labels 的值重新初始化模型的最后一个分类层，从而忽略与预训练模型不匹配的维度。这意味着当你指定 num_labels 后，模型的最后一个分类层会根据新的类别数进行初始化，而其他层的权重会从预训练模型中加载。</li></ul><figure class="highlight python"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Some weights of DistilBertForSequenceClassification were <span class="keyword">not</span> initialized <span class="keyword">from</span> the model checkpoint at distilbert-base-uncased <span class="keyword">and</span> are newly initialized: [<span class="string">&#x27;classifier.bias&#x27;</span>, <span class="string">&#x27;classifier.weight&#x27;</span>, <span class="string">&#x27;pre_classifier.bias&#x27;</span>, <span class="string">&#x27;pre_classifier.weight&#x27;</span>]</span><br><span class="line">You should probably TRAIN this model on a down-stream task to be able to use it <span class="keyword">for</span> predictions <span class="keyword">and</span> inference.</span><br><span class="line"></span><br><span class="line">Total parameters: <span class="number">66961931</span></span><br><span class="line">Trainable parameters: <span class="number">66961931</span></span><br><span class="line"></span><br><span class="line">=========================================================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                                  Output Shape              Param <span class="comment">#</span></span><br><span class="line">=========================================================================================================</span><br><span class="line">DistilBertForSequenceClassification                     [<span class="number">1</span>, <span class="number">11</span>]                   --</span><br><span class="line">├─DistilBertModel: <span class="number">1</span>-<span class="number">1</span>                                  [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    └─Embeddings: <span class="number">2</span>-<span class="number">1</span>                                  [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    │    └─Embedding: <span class="number">3</span>-<span class="number">1</span>                              [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               <span class="number">23</span>,<span class="number">440</span>,<span class="number">896</span></span><br><span class="line">│    │    └─Embedding: <span class="number">3</span>-<span class="number">2</span>                              [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               <span class="number">393</span>,<span class="number">216</span></span><br><span class="line">│    │    └─LayerNorm: <span class="number">3</span>-<span class="number">3</span>                              [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               <span class="number">1</span>,<span class="number">536</span></span><br><span class="line">│    │    └─Dropout: <span class="number">3</span>-<span class="number">4</span>                                [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    └─Transformer: <span class="number">2</span>-<span class="number">2</span>                                 [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    │    └─ModuleList: <span class="number">3</span>-<span class="number">5</span>                             --                        <span class="number">42</span>,<span class="number">527</span>,<span class="number">232</span></span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">2</span>                                           [<span class="number">1</span>, <span class="number">768</span>]                  <span class="number">590</span>,<span class="number">592</span></span><br><span class="line">├─Dropout: <span class="number">1</span>-<span class="number">3</span>                                          [<span class="number">1</span>, <span class="number">768</span>]                  --</span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">4</span>                                           [<span class="number">1</span>, <span class="number">11</span>]                   <span class="number">8</span>,<span class="number">459</span></span><br><span class="line">=========================================================================================================</span><br><span class="line">Total params: <span class="number">66</span>,<span class="number">961</span>,<span class="number">931</span></span><br><span class="line">Trainable params: <span class="number">66</span>,<span class="number">961</span>,<span class="number">931</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">Total mult-adds (M): <span class="number">66.96</span></span><br><span class="line">=========================================================================================================</span><br><span class="line">Input size (MB): <span class="number">0.00</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">2.97</span></span><br><span class="line">Params size (MB): <span class="number">267.85</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">270.82</span></span><br><span class="line">=========================================================================================================</span><br></pre></td></tr></table></figure><h3 id="Training-Arguments"><a href="#Training-Arguments" class="headerlink" title="Training Arguments"></a>Training Arguments</h3><p>定义训练参数，都是一些比较常见的参数。</p><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=OUT_DIR,</span><br><span class="line">    learning_rate=LR,</span><br><span class="line">    per_device_train_batch_size=BATCH_SIZE,</span><br><span class="line">    per_device_eval_batch_size=BATCH_SIZE,</span><br><span class="line">    num_train_epochs=EPOCHS,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    eval_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    load_best_model_at_end=<span class="literal">True</span>,</span><br><span class="line">    save_total_limit=<span class="number">3</span>,</span><br><span class="line">    report_to=<span class="string">&#x27;tensorboard&#x27;</span>,</span><br><span class="line">    fp16=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Initializing-the-Trainer"><a href="#Initializing-the-Trainer" class="headerlink" title="Initializing the Trainer"></a>Initializing the Trainer</h3><p>实例化一个 <code>Trainer</code>，并开始训练。</p><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=tokenized_train,</span><br><span class="line">    eval_dataset=tokenized_valid,</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">history = trainer.train()</span><br></pre></td></tr></table></figure><blockquote><p>写的时候在自己电脑上跑的，跑太慢了，就没继续跑下去了，但是这个代码是可以正常运行的。</p></blockquote><h3 id="Evaluation-Inference-on-Unseen-Data"><a href="#Evaluation-Inference-on-Unseen-Data" class="headerlink" title="Evaluation &amp;&amp; Inference on Unseen Data"></a>Evaluation &amp;&amp; Inference on Unseen Data</h3><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">trainer.evaluate(tokenized_test)</span><br><span class="line"></span><br><span class="line">AutoModelForSequenceClassification.from_pretrained(<span class="string">f&quot;arxiv_bert/checkpoint-4440&quot;</span>)</span><br><span class="line"> </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">classify = pipeline(task=<span class="string">&#x27;text-classification&#x27;</span>, model=model, tokenizer=tokenizer)</span><br><span class="line"> </span><br><span class="line">all_files = glob.glob(<span class="string">&#x27;inference_data/*&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> file_name <span class="keyword">in</span> all_files:</span><br><span class="line">    file = <span class="built_in">open</span>(file_name)</span><br><span class="line">    content = file.read()</span><br><span class="line">    <span class="built_in">print</span>(content)</span><br><span class="line">    result = classify(content)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;PRED: &#x27;</span>, result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;GT: &#x27;</span>, file_name.split(<span class="string">&#x27;_&#x27;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.txt&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">使用 Hugging Face Transformers Fine-Tune BERT。</summary>
    
    
    
    
    <category term="nlp" scheme="http://example.com/tag/nlp/"/>
    
    <category term="转载" scheme="http://example.com/tag/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
</feed>
