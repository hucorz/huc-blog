<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="John Doe">





<title>使用 Hugging Face Transformers Fine-Tune BERT | Huc&#39;s Blog</title>



<link rel="icon" href="/images/logo-removebg.png">



<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/nprogress/nprogress.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>


<script src="/lib/nprogress/nprogress.js"></script>

<script>
  $(document).ready(() => {
    NProgress.configure({
      showSpinner: false,
    });
    NProgress.start();
    $("#nprogress .bar").css({
      background: "#de7441",
    });
    $("#nprogress .peg").css({
      "box-shadow": "0 0 2px #de7441, 0 0 4px #de7441",
    });
    $("#nprogress .spinner-icon").css({
      "border-top-color": "#de7441",
      "border-left-color": "#de7441",
    });
    setTimeout(function () {
      NProgress.done();
      $(".fade").removeClass("out");
    }, 800);
  });
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }

    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ri:moon-line" : "ri:sun-line"
          );
          toggleGiscusTheme();
        }
      });

    $("#toggle-dark").click((event) => {
      const isAppearanceTransition = document.startViewTransition && !window.matchMedia('(prefers-reduced-motion: reduce)').matches
      if (!isAppearanceTransition) {
        toggleDark()
        return
      }
      const x = event.clientX
      const y = event.clientY
      const endRadius = Math.hypot(
        Math.max(x, innerWidth - x),
        Math.max(y, innerHeight - y),
      )
      const transition = document.startViewTransition(async () => {
        toggleDark()
      })

      transition.ready
        .then(() => {
          const isDark = document.documentElement.classList.contains("dark")
          const clipPath = [
            `circle(0px at ${x}px ${y}px)`,
            `circle(${endRadius}px at ${x}px ${y}px)`,
          ]
          document.documentElement.animate(
            {
              clipPath: isDark
                ? [...clipPath].reverse()
                : clipPath,
            },
            {
              duration: 400,
              easing: 'ease-out',
              pseudoElement: isDark
                ? '::view-transition-old(root)'
                : '::view-transition-new(root)',
            },
          )
        })
    });
  });
</script>




<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Huc's Blog" type="application/atom+xml">
<link rel="alternate" href="/rss2.xml" title="Huc's Blog" type="application/rss+xml">
</head>
<body class="font-sans bg-white dark:bg-zinc-900 text-gray-700 dark:text-gray-200 relative">
  <header class="fixed w-full px-5 py-1 z-10 backdrop-blur-xl backdrop-saturate-150 border-b border-black/5">
  <div class="max-auto">
    <nav class="flex items-center text-base">
      <a href="/" class="group">
        <h2 class="font-medium tracking-tighterp text-l p-2">
          <img class="w-5 mr-2 inline-block transition-transform group-hover:rotate-[30deg]" id="logo" src="/images/logo-removebg.png" alt="Huc's Blog" />
          Huc&#39;s Blog
        </h2>
      </a>
      <div id="header-title" class="opacity-0 md:ml-2 md:mt-[0.1rem] text-xs font-medium whitespace-nowrap overflow-hidden overflow-ellipsis">
        使用 Hugging Face Transformers Fine-Tune BERT
      </div>
      <div class="flex-1"></div>
      <div class="flex items-center gap-3">
        
          <a class="hidden sm:flex" href="/archives">Posts</a>
        
          <a class="hidden sm:flex" href="/tag">Tags</a>
        
        
          
            <a class="w-5 h-5 hidden sm:flex" title="Github" target="_blank" rel="noopener" href="https://github.com/hucorz">
              <iconify-icon width="20" icon="ri:github-line"></iconify-icon>
            </a>
          
        
        <a class="w-5 h-5 hidden sm:flex" title="Github" href="rss2.xml">
          <iconify-icon width="20" icon="ri:rss-line"></iconify-icon>
        </a>
        <a class="w-5 h-5" title="toggle theme" id="toggle-dark">
          <iconify-icon width="20" icon="" id="theme-icon"></iconify-icon>
        </a>
      </div>
      <div class="flex items-center justify-center gap-3 ml-3 sm:hidden">
        <span class="w-5 h-5" aria-hidden="true" role="img" id="open-menu">
          <iconify-icon width="20" icon="carbon:menu" ></iconify-icon>
        </span>
        <span class="w-5 h-5 hidden" aria-hidden="true" role="img" id="close-menu">
          <iconify-icon  width="20" icon="carbon:close" ></iconify-icon>
        </span>
      </div>
    </nav>
  </div>
</header>
<div id="menu-panel" class="h-0 overflow-hidden sm:hidden fixed left-0 right-0 top-12 bottom-0 z-10">
  <div id="menu-content" class="relative z-20 bg-white/80 px-6 sm:px-8 py-2 backdrop-blur-xl -translate-y-full transition-transform duration-300">
    <ul class="nav flex flex-col sm:flex-row text-sm font-medium">
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/archives" class="flex h-12 sm:h-auto items-center">Posts</a>
        </li>
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/tag" class="flex h-12 sm:h-auto items-center">Tags</a>
        </li>
      
    </ul>
  </div>
  <div class="mask bg-black/20 absolute inset-0"></div>
</div>

  <main class="pt-14">
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">


<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

<!-- toc -->

  <!-- tocbot -->
<nav class="post-toc toc text-sm w-48 relative top-32 right-0 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- header -->
  <header class="overflow-hidden pt-6 pb-6 md:pt-12">
    <div class="pt-4 md:pt-6">
      <h1 id="article-title" class="text-[2rem] font-bold leading-snug mb-4 md:mb-6 md:text-[2.6rem]">
        使用 Hugging Face Transformers Fine-Tune BERT
      </h1>
      <!-- tag -->
      <div class="pb-6">
        
          
            <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
              <a href="/tag/nlp/">nlp</a>
            </span>
          
            <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
              <a href="/tag/%E8%BD%AC%E8%BD%BD/">转载</a>
            </span>
          
        
      </div>
      <div>
        <section class="flex items-center gap-3 text-sm">
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="carbon-calendar" ></iconify-icon>
            <time>2024-08-31</time>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="ic:round-access-alarm" ></iconify-icon>
            <span>8 min</span>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="icon-park-outline:font-search" ></iconify-icon>
            <span>1.8k words</span>
          </span>
          
        </section>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto slide-enter-content dark:prose-invert">
    <h1 id="Fine-Tuning-BERT-using-Hugging-Face-Transformers"><a href="#Fine-Tuning-BERT-using-Hugging-Face-Transformers" class="headerlink" title="Fine-Tuning BERT using Hugging Face Transformers"></a>Fine-Tuning BERT using Hugging Face Transformers</h1><p><a target="_blank" rel="noopener" href="https://learnopencv.com/fine-tuning-bert/">原文链接</a></p>
<p>本篇文章使用 Hugging Face Transformers Fine-Tune BERT，获得一个能够对 Arxiv 文章摘要进行分类的模型，一共 11 个类别。</p>
<p>本篇文章包含以下内容：</p>
<ul>
<li>安装依赖</li>
<li>解释用来 Fine-Tune BERT 的数据集</li>
<li>模型和训练</li>
<li>模型测试和评估</li>
</ul>
<blockquote>
<p>本文大部分用到的 api 在 <code>transformer</code> 的<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/index">官方文档</a>都有，这里仅转载一下主要的处理流程。</p>
<p>如果实践中数据集是没有 label 的，则需要使用无监督的方法，比如 Masked Language Modeling（MLM） 以及 Next Sentence Prediction（NSP）。</p>
<p>为了构造 MLM 和 NSP 的数据，可以使用 transformers 库中的 <code>DataCollatorForLanguageModeling</code> 和 <code>DataCollatorForNextSentencePrediction</code>。</p>
</blockquote>
<h2 id="Why-Fine-Tuning-BERT-Matters"><a href="#Why-Fine-Tuning-BERT-Matters" class="headerlink" title="Why Fine-Tuning BERT Matters?"></a>Why Fine-Tuning BERT Matters?</h2><p>Fine-Tuning 的目的是为了让 BERT 从一个通用的工具变成一个特定任务的工具。</p>
<h2 id="Hugging-Face-Transformers"><a href="#Hugging-Face-Transformers" class="headerlink" title="Hugging Face Transformers"></a>Hugging Face Transformers</h2><p>本文章使用 Hugging Face Transformers，一个用于自然语言处理的库。</p>
<h2 id="Fine-Tuning-BERT-on-the-Arxiv-Abstract-Classification-Dataset"><a href="#Fine-Tuning-BERT-on-the-Arxiv-Abstract-Classification-Dataset" class="headerlink" title="Fine-Tuning BERT on the Arxiv Abstract Classification Dataset"></a>Fine-Tuning BERT on the Arxiv Abstract Classification Dataset</h2><p>首先，我们需要安装一些依赖。</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!pip install -U transformers datasets evaluate accelerate</span><br><span class="line">!pip install scikit-learn</span><br><span class="line">!pip install tensorboard</span><br></pre></td></tr></table></figure>

<h3 id="Importing-the-Necessary-Libraries"><a href="#Importing-the-Necessary-Libraries" class="headerlink" title="Importing the Necessary Libraries"></a>Importing the Necessary Libraries</h3><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    AutoTokenizer,</span><br><span class="line">    DataCollatorWithPadding,</span><br><span class="line">    AutoModelForSequenceClassification,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">    Trainer,</span><br><span class="line">    pipeline,</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h3 id="Setting-the-Hyperparameters-for-Fine-Tuning-BERT"><a href="#Setting-the-Hyperparameters-for-Fine-Tuning-BERT" class="headerlink" title="Setting the Hyperparameters for Fine-Tuning BERT"></a>Setting the Hyperparameters for Fine-Tuning BERT</h3><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">NUM_PROCS = <span class="number">32</span></span><br><span class="line">LR = <span class="number">0.00005</span></span><br><span class="line">EPOCHS = <span class="number">5</span></span><br><span class="line">MODEL = <span class="string">&#x27;distilbert-base-uncased&#x27;</span></span><br><span class="line">OUT_DIR = <span class="string">&#x27;arxiv_bert&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>MODEL：表示我们要使用的模型，这里使用的是 <code>bert-base-uncased</code>。这里的 <code>uncased</code> 表示该模型是在小写的文本上训练的。</li>
<li>OUT_DIR：表示我们训练好的模型的保存路径。</li>
</ul>
<h3 id="Download-and-Prepare-the-Dataset"><a href="#Download-and-Prepare-the-Dataset" class="headerlink" title="Download and Prepare the Dataset"></a>Download and Prepare the Dataset</h3><p>使用 <code>datasets</code> 库下载数据集 arxiv 的分类数据集。</p>
<figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = load_dataset(<span class="string">&quot;ccdv/arxiv-classification&quot;</span>, split=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">valid_dataset = load_dataset(<span class="string">&quot;ccdv/arxiv-classification&quot;</span>, split=<span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line">test_dataset = load_dataset(<span class="string">&quot;ccdv/arxiv-classification&quot;</span>, split=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_dataset)</span><br><span class="line"><span class="built_in">print</span>(valid_dataset)</span><br><span class="line"><span class="built_in">print</span>(test_dataset)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>下载数据可能需要为 load_dataset 加一个 trust_remote_code&#x3D;True 的参数。</p>
</blockquote>
<figure class="highlight cmd"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line"><span class="function">    features: [&#x27;<span class="title">text</span>&#x27;, &#x27;<span class="title">label</span>&#x27;],</span></span><br><span class="line"><span class="function">    <span class="title">num_rows</span>: 28388</span></span><br><span class="line"><span class="function">&#125;)</span></span><br><span class="line"><span class="function"><span class="title">Dataset</span>(&#123;</span></span><br><span class="line"><span class="function">    <span class="title">features</span>: [&#x27;<span class="title">text</span>&#x27;, &#x27;<span class="title">label</span>&#x27;],</span></span><br><span class="line"><span class="function">    <span class="title">num_rows</span>: 2500</span></span><br><span class="line"><span class="function">&#125;)</span></span><br><span class="line"><span class="function"><span class="title">Dataset</span>(&#123;</span></span><br><span class="line"><span class="function">    <span class="title">features</span>: [&#x27;<span class="title">text</span>&#x27;, &#x27;<span class="title">label</span>&#x27;],</span></span><br><span class="line"><span class="function">    <span class="title">num_rows</span>: 2500</span></span><br><span class="line"><span class="function">&#125;)</span></span><br></pre></td></tr></table></figure>

<p>查看一下 dataset 的格式，可以看到每个样本有两个字段，一个是 text，一个是 label。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Constrained Submodular Maximization ... and A.14.\n\n22\n\n\x0c&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;label&#x27;</span>: <span class="number">8</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Dataset-Label-Information"><a href="#Dataset-Label-Information" class="headerlink" title="Dataset Label Information"></a>Dataset Label Information</h3><p>定义两个字典，用来将 label 转换为对应的类别，以及将类别转换为对应的 label。</p>
<figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">id2label = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;math.AC&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;cs.CV&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;cs.AI&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;cs.SY&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;math.GR&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;cs.CE&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;cs.PL&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;cs.IT&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;cs.DS&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;cs.NE&quot;</span>,</span><br><span class="line">    <span class="number">10</span>: <span class="string">&quot;math.ST&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">label2id = &#123;</span><br><span class="line">    <span class="string">&quot;math.AC&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;cs.CV&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;cs.AI&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;cs.SY&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;math.GR&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;cs.CE&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;cs.PL&quot;</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">&quot;cs.IT&quot;</span>: <span class="number">7</span>,</span><br><span class="line">    <span class="string">&quot;cs.DS&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&quot;cs.NE&quot;</span>: <span class="number">9</span>,</span><br><span class="line">    <span class="string">&quot;math.ST&quot;</span>: <span class="number">10</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Tokenizing-the-Dataset"><a href="#Tokenizing-the-Dataset" class="headerlink" title="Tokenizing the Dataset"></a>Tokenizing the Dataset</h3><p>使用 <code>AutoTokenizer</code> 实例化一个 tokenizer，并对不同的 dataset 进行 tokenization。</p>
<figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(MODEL)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">tokenizer, examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;text&quot;</span>],</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">partial_tokenize_function = partial(preprocess_function, tokenizer)</span><br><span class="line"></span><br><span class="line">tokenized_train = train_dataset.<span class="built_in">map</span>(</span><br><span class="line">    partial_tokenize_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    num_proc=NUM_PROCS</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">tokenized_valid = valid_dataset.<span class="built_in">map</span>(</span><br><span class="line">    partial_tokenize_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    num_proc=NUM_PROCS</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">tokenized_test = test_dataset.<span class="built_in">map</span>(</span><br><span class="line">    partial_tokenize_function,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    num_proc=NUM_PROCS</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>

<p>给 <code>AutoTokenizer</code> 传递的模型名能够初始化专门针对该模型的 tokenizer。</p>
<p><code>DataCollatorWithPadding</code> 会在训练时动态地将不同长度的 token 转换为相同长度的 tensor。</p>
<blockquote>
<p>Transformers 的 <code>Tokenizer</code> 支持 truncation 和 padding 参数，具体查看 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">transformers.PreTrainedTokenizer</a></p>
<p>tokenizer 和 DataCollatorWithPadding 的 padding 之间的区别：</p>
<ul>
<li>tokenizer 的 padding 是在 tokenization 时使用，用来将不同长度的文本转换为相同长度的 token。</li>
<li>DataCollatorWithPadding 的 padding 是在 batch 生成时动态地来将不同长度的 token 转换为相同长度的 tensor。</li>
</ul>
<p>DataCollatorWithPadding 传入 tokenizer 需要利用 tokenzier 的信息进行正确地 padding </p>
<p>使用原文中的代码我遇到了一个 Tokenizer is not defined 的报错，按照 <a target="_blank" rel="noopener" href="https://discuss.huggingface.co/t/tokenizer-is-not-defined/39231">这里</a> 的解决方法，用 partial 函数解决了这个问题。</p>
</blockquote>
<h3 id="Sample-Tokenization-Example"><a href="#Sample-Tokenization-Example" class="headerlink" title="Sample Tokenization Example"></a>Sample Tokenization Example</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenized_sample = partial_tokenize_function(train_dataset[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(tokenized_sample)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of tokenized IDs: <span class="subst">&#123;<span class="built_in">len</span>(tokenized_sample.input_ids)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of attention mask: <span class="subst">&#123;<span class="built_in">len</span>(tokenized_sample.attention_mask)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">101</span>, <span class="number">14924</span>, <span class="number">2635</span>, <span class="number">24421</span>, <span class="number">13997.</span>.. <span class="number">102</span>], </span><br><span class="line"><span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, ... <span class="number">0</span>],</span><br><span class="line"><span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ... <span class="number">1</span>]&#125;</span><br><span class="line">Length of tokenized IDs: <span class="number">512</span></span><br><span class="line">Length of attention mask: <span class="number">512</span></span><br></pre></td></tr></table></figure>

<ul>
<li>input_ids：表示 tokenized 后的文本，每个 token 都有一个对应的 id。例如，101 表示 [CLS]，表示序列的开始；102 表示 [SEP]，表示序列的结束。</li>
<li>token_type_ids：表示 token 的类型，BERT 用于区分两个句子的 token。</li>
<li>attention_mask：表示哪些 token 是 padding 的，哪些是真实的 token。</li>
</ul>
<h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><p>定义一个函数，用来计算模型的评估指标。</p>
<figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">accuracy = evaluate.load(<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_pred</span>):</span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> accuracy.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure>

<h3 id="Preparing-the-BERT-Model"><a href="#Preparing-the-BERT-Model" class="headerlink" title="Preparing the BERT Model"></a>Preparing the BERT Model</h3><p>实例化一个 11 分类的 BERT 模型，最后的 BERT 模型有 0.67 亿参数。</p>
<figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModelForSequenceClassification.from_pretrained(</span><br><span class="line">    MODEL,</span><br><span class="line">    num_labels=<span class="number">11</span>,</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id,</span><br><span class="line">)</span><br><span class="line">model.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line">total_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters())</span><br><span class="line">trainable_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nTotal parameters: <span class="subst">&#123;total_params&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Trainable parameters: <span class="subst">&#123;trainable_params&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">dummy_input = tokenizer(<span class="string">&quot;This is a dummy input&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">dummy_input_dict = &#123;k: v.to(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">for</span> k, v <span class="keyword">in</span> dummy_input.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">summary(model, input_data=dummy_input_dict)</span><br></pre></td></tr></table></figure>

<ul>
<li>num_labels：表示分类任务中的类别数。transformers 在加载预训练模型的权重时，会根据 num_labels 的值重新初始化模型的最后一个分类层，从而忽略与预训练模型不匹配的维度。这意味着当你指定 num_labels 后，模型的最后一个分类层会根据新的类别数进行初始化，而其他层的权重会从预训练模型中加载。</li>
</ul>
<figure class="highlight python"><figcaption><span>filename</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Some weights of DistilBertForSequenceClassification were <span class="keyword">not</span> initialized <span class="keyword">from</span> the model checkpoint at distilbert-base-uncased <span class="keyword">and</span> are newly initialized: [<span class="string">&#x27;classifier.bias&#x27;</span>, <span class="string">&#x27;classifier.weight&#x27;</span>, <span class="string">&#x27;pre_classifier.bias&#x27;</span>, <span class="string">&#x27;pre_classifier.weight&#x27;</span>]</span><br><span class="line">You should probably TRAIN this model on a down-stream task to be able to use it <span class="keyword">for</span> predictions <span class="keyword">and</span> inference.</span><br><span class="line"></span><br><span class="line">Total parameters: <span class="number">66961931</span></span><br><span class="line">Trainable parameters: <span class="number">66961931</span></span><br><span class="line"></span><br><span class="line">=========================================================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                                  Output Shape              Param <span class="comment">#</span></span><br><span class="line">=========================================================================================================</span><br><span class="line">DistilBertForSequenceClassification                     [<span class="number">1</span>, <span class="number">11</span>]                   --</span><br><span class="line">├─DistilBertModel: <span class="number">1</span>-<span class="number">1</span>                                  [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    └─Embeddings: <span class="number">2</span>-<span class="number">1</span>                                  [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    │    └─Embedding: <span class="number">3</span>-<span class="number">1</span>                              [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               <span class="number">23</span>,<span class="number">440</span>,<span class="number">896</span></span><br><span class="line">│    │    └─Embedding: <span class="number">3</span>-<span class="number">2</span>                              [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               <span class="number">393</span>,<span class="number">216</span></span><br><span class="line">│    │    └─LayerNorm: <span class="number">3</span>-<span class="number">3</span>                              [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               <span class="number">1</span>,<span class="number">536</span></span><br><span class="line">│    │    └─Dropout: <span class="number">3</span>-<span class="number">4</span>                                [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    └─Transformer: <span class="number">2</span>-<span class="number">2</span>                                 [<span class="number">1</span>, <span class="number">7</span>, <span class="number">768</span>]               --</span><br><span class="line">│    │    └─ModuleList: <span class="number">3</span>-<span class="number">5</span>                             --                        <span class="number">42</span>,<span class="number">527</span>,<span class="number">232</span></span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">2</span>                                           [<span class="number">1</span>, <span class="number">768</span>]                  <span class="number">590</span>,<span class="number">592</span></span><br><span class="line">├─Dropout: <span class="number">1</span>-<span class="number">3</span>                                          [<span class="number">1</span>, <span class="number">768</span>]                  --</span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">4</span>                                           [<span class="number">1</span>, <span class="number">11</span>]                   <span class="number">8</span>,<span class="number">459</span></span><br><span class="line">=========================================================================================================</span><br><span class="line">Total params: <span class="number">66</span>,<span class="number">961</span>,<span class="number">931</span></span><br><span class="line">Trainable params: <span class="number">66</span>,<span class="number">961</span>,<span class="number">931</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">Total mult-adds (M): <span class="number">66.96</span></span><br><span class="line">=========================================================================================================</span><br><span class="line">Input size (MB): <span class="number">0.00</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">2.97</span></span><br><span class="line">Params size (MB): <span class="number">267.85</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">270.82</span></span><br><span class="line">=========================================================================================================</span><br></pre></td></tr></table></figure>

<h3 id="Training-Arguments"><a href="#Training-Arguments" class="headerlink" title="Training Arguments"></a>Training Arguments</h3><p>定义训练参数，都是一些比较常见的参数。</p>
<figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=OUT_DIR,</span><br><span class="line">    learning_rate=LR,</span><br><span class="line">    per_device_train_batch_size=BATCH_SIZE,</span><br><span class="line">    per_device_eval_batch_size=BATCH_SIZE,</span><br><span class="line">    num_train_epochs=EPOCHS,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    eval_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    load_best_model_at_end=<span class="literal">True</span>,</span><br><span class="line">    save_total_limit=<span class="number">3</span>,</span><br><span class="line">    report_to=<span class="string">&#x27;tensorboard&#x27;</span>,</span><br><span class="line">    fp16=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="Initializing-the-Trainer"><a href="#Initializing-the-Trainer" class="headerlink" title="Initializing the Trainer"></a>Initializing the Trainer</h3><p>实例化一个 <code>Trainer</code>，并开始训练。</p>
<figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=tokenized_train,</span><br><span class="line">    eval_dataset=tokenized_valid,</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">history = trainer.train()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>写的时候在自己电脑上跑的，跑太慢了，就没继续跑下去了，但是这个代码是可以正常运行的。</p>
</blockquote>
<h3 id="Evaluation-Inference-on-Unseen-Data"><a href="#Evaluation-Inference-on-Unseen-Data" class="headerlink" title="Evaluation &amp;&amp; Inference on Unseen Data"></a>Evaluation &amp;&amp; Inference on Unseen Data</h3><figure class="highlight python"><figcaption><span>showLineNumbers</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">	</span><br><span class="line">trainer.evaluate(tokenized_test)</span><br><span class="line"></span><br><span class="line">AutoModelForSequenceClassification.from_pretrained(<span class="string">f&quot;arxiv_bert/checkpoint-4440&quot;</span>)</span><br><span class="line"> </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">classify = pipeline(task=<span class="string">&#x27;text-classification&#x27;</span>, model=model, tokenizer=tokenizer)</span><br><span class="line"> </span><br><span class="line">all_files = glob.glob(<span class="string">&#x27;inference_data/*&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> file_name <span class="keyword">in</span> all_files:</span><br><span class="line">    file = <span class="built_in">open</span>(file_name)</span><br><span class="line">    content = file.read()</span><br><span class="line">    <span class="built_in">print</span>(content)</span><br><span class="line">    result = classify(content)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;PRED: &#x27;</span>, result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;GT: &#x27;</span>, file_name.split(<span class="string">&#x27;_&#x27;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.txt&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>









  </article>
  <!-- prev and next -->
  <div class="flex justify-between mt-12 pt-6 border-t border-gray-200">
    <div>
      
        <a href="/2024/08/29/hello-world/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          <iconify-icon width="20" icon="ri:arrow-left-s-line" data-inline="false"></iconify-icon>
          Hello World
        </a>
      
    </div>
    <div>
      
    </div>
  </div>
  <!-- comment -->
  <div class="article-comments mt-12">
    

  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->


  </main>
  <footer class="flex flex-col h-40 items-center justify-center text-gray-400 text-sm">
  <!-- busuanzi -->
  
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex items-center gap-2">
  <span>Visitors</span>
  <span id="busuanzi_value_site_uv"></span>
  <span>Page Views</span>
  <span id="busuanzi_value_site_pv"></span>
</div>
<!-- End Busuanzi Analytics -->


  <!-- copyright -->
  <div class="flex items-center gap-2">
    <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" style="color: inherit;">CC BY-NC-SA 4.0</a>
    <span>© 2022</span>
    <iconify-icon width="18" icon="emojione-monotone:maple-leaf" ></iconify-icon>
    <a href="https://github.com/hucorz" target="_blank" rel="noopener noreferrer">hucorz</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-2">
    <span>Powered by</span>
    <a href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/xbmlz/hexo-theme-maple" target="_blank" rel="noopener noreferrer">Maple</a>
  </div>

</footer>

  <div class="back-to-top box-border fixed right-6 z-1024 -bottom-20 rounded py-1 px-1 bg-slate-900 opacity-60 text-white cursor-pointer text-center dark:bg-slate-600">
    <span class="flex justify-center items-center text-sm">
      <iconify-icon width="18" icon="ion:arrow-up-c" id="go-top"></iconify-icon>
      <span id="scrollpercent"><span>0</span> %</span>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <script>
    $(document).ready(function () {
      const mapleCount = "10";
      const speed = "0.5";
      const mapleEl = document.getElementById("maple");
      const maples = Array.from({ length: mapleCount }).map(() => {
        const maple = document.createElement("div");
        const scale = Math.random() * 0.5 + 0.5;
        const offset = Math.random() * 2 - 1;
        const x = Math.random() * mapleEl.clientWidth;
        const y = -Math.random() * mapleEl.clientHeight;
        const duration = 10 / speed;
        const delay = -duration;
        maple.className = "maple";
        maple.style.width = `${24 * scale}px`;
        maple.style.height = `${24 * scale}px`;
        maple.style.left = `${x}px`;
        maple.style.top = `${y}px`;
        maple.style.setProperty("--maple-fall-offset", offset);
        maple.style.setProperty("--maple-fall-height", `${Math.abs(y) + mapleEl.clientHeight}px`);
        maple.style.animation = `fall ${duration}s linear infinite`;
        maple.style.animationDelay = `${delay}s`;
        mapleEl.appendChild(maple)
        return maple
      })
    });
  </script>
  


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
</body>

</html>
